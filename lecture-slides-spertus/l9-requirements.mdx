---
sidebar_position: 9
title: "Interpreting Requirements"
image: /img/lectures/web/l9.png
---

import RevealJS, { Slide } from '@site/src/components/RevealJS';
import Img from '@site/src/components/Img';
import PollSlide from '@site/src/components/PollSlide';

<RevealJS transition="slide">

{/* ============================================ */}
{/* COVER IMAGE */}
{/* ============================================ */}

<Slide>
  <Img
    src="/img/lectures/web/l9.png"
    prompt="Concept: 'Interpreting Requirements' (Program Design & Implementation). A pixel art educational illustration showing the classic 'tire swing' communication problem. On the left, a customer describes what they want (a simple tree swing). In the middle, various interpretations appear as thought bubbles over different people: a developer sees a complex rope system, a designer sees a modern art installation, a tester sees a safety-padded swing. On the right, the actual delivered product is something entirely different. The scene shows the importance of clear communication and requirements analysis. Include speech bubbles with '???' to show confusion. Tagline: 'What They Said. What We Heard. What They Needed.' Retro pixel art style with teals, blues, purples, and warm accents."
    alt="A pixel art illustration showing the requirements communication problem. A customer describes a tree swing, but different team members imagine different interpretations - a developer sees complex ropes, a designer sees art, a tester sees safety features. The delivered product differs from all expectations. Tagline: What They Said. What We Heard. What They Needed."
  />

<aside className="notes">
**Lecture overview:**
- **Total time:** ~55 minutes
- **Prerequisites:** Students understand design principles from L7-L8
- **Connects to:** Assignment 3 (requirements analysis practice)

**Structure:**
- Purpose of requirements analysis (~10 min)
- Three dimensions of risk (~15 min)
- Stakeholder identification (~15 min)
- Elicitation methods (~15 min)

**Key theme:** Requirements analysis is about understanding problems deeply enough to design solutions stakeholders didn't know were possible.

→ **Transition:** Let's start with the learning objectives...
</aside>

</Slide>

{/* ============================================ */}
{/* TITLE SLIDE */}
{/* ============================================ */}

<Slide>

# CS 3100: Program Design and Implementation II

## Lecture 9: Interpreting Requirements

<p style={{marginTop: '2em', fontSize: '0.8em', color: '#666'}}>
  ©2025 Jonathan Bell, CC-BY-SA
</p>

<aside className="notes">
**Context from L7-L8:**
- Students learned about changeability and SOLID principles
- They understand how design decisions affect maintainability
- This lecture shifts focus from *how* to build to *what* to build

**Key theme:** Even perfect code is worthless if it solves the wrong problem.

→ **Transition:** Here's what you'll be able to do after today...
</aside>

</Slide>

{/* ============================================ */}
{/* LEARNING OBJECTIVES */}
{/* ============================================ */}

<Slide>

## Learning Objectives

<p style={{fontSize: '0.85em', textAlign: 'left'}}>
After this lecture, you will be able to:
</p>

<ol style={{fontSize: '0.75em', textAlign: 'left'}}>
  <li>Explain the overall purpose of requirements analysis and why it matters</li>
  <li>Enumerate and explain three major dimensions of risk in requirements analysis</li>
  <li>Identify the stakeholders of a software module, along with their values and interests</li>
  <li>Describe multiple methods to elicit users' requirements</li>
</ol>

<aside className="notes">
**Time allocation:**
- Objective 1: Purpose of requirements (~10 min)
- Objective 2: Risk dimensions (~15 min)
- Objective 3: Stakeholder analysis (~15 min)
- Objective 4: Elicitation methods (~15 min)

**Why this matters:** Most failed projects fail because of requirements problems, not technical problems.

→ **Transition:** Let's start with a story...
</aside>

</Slide>

{/* ============================================ */}
{/* ARC 1: PURPOSE OF REQUIREMENTS ANALYSIS */}
{/* ============================================ */}

<Slide>

## "I Need a System to Help My TAs Grade More Efficiently"

<p style={{fontSize: '1.1em', marginTop: '1em', fontStyle: 'italic', color: '#9370DB'}}>
"Can you build something by next month?"
</p>

<p style={{fontSize: '0.9em', marginTop: '1em'}}>
Sounds straightforward, right? But consider:
</p>

<ul style={{fontSize: '0.85em'}}>
  <li>What does <strong>"efficiently"</strong> mean?</li>
  <li>What specific <strong>problems</strong> are TAs facing?</li>
  <li>What does <strong>"grading"</strong> include?</li>
  <li>Is it just running tests, or also feedback, regrades, statistics?</li>
</ul>

<aside className="notes">
**The opening scenario:**
- Sounds straightforward, right?
- But what does "efficiently" mean?
- Is grading just running tests, or also feedback, regrades, statistics?

**Key point:** Vague requirements lead to wrong solutions.

→ **Transition:** Let's see what happens without proper requirements analysis...
</aside>

</Slide>

<Slide>

## What the Developer Built vs. What Was Actually Needed

<div style={{display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '1em', fontSize: '0.7em'}}>

<div>

**Version 1: What was built**
- Run test cases on submissions
- Return a numerical score
- Store the final grade

</div>

<div>

**Version 2: What was needed**
- Run automated tests on code
- Check code style compliance
- Enable inline commenting on code
- Preserve previous grading on resubmit
- Notify original graders of resubmissions
- Prevent same grader from repeatedly grading same student
- Randomly sample 10% for quality review
- Track grader workload for fair distribution
- Support multi-stage regrade requests
- Integrate with university grade system

</div>

</div>

<p style={{fontSize: '0.8em', marginTop: '0.5em', color: '#FF9800'}}>
⚠ The difference: hundreds of hours of wasted work, frustrated users, missed deadlines.
</p>

<aside className="notes">
**The gap:**
- Version 1 is technically correct but useless
- Version 2 addresses real problems
- This gap represents massive wasted effort

**Key insight:** Requirements analysis bridges "make grading better" to concrete features.

→ **Transition:** This problem has existed for over a century...
</aside>

</Slide>

<Slide>

## The Requirements Communication Problem Is Not New

<Img src="/img/tire-swing-oregon-experimentp44.jpg" alt="The classic tire swing cartoon showing how requirements get lost in translation: what the customer described, what was designed, what was built, and what the customer actually needed." style={{maxHeight: '450px'}} />

<p style={{fontSize: '0.75em', textAlign: 'center', fontStyle: 'italic'}}>
Versions of this image have supposedly circulated since the early 1900s.
Christopher Alexander used it in <em>The Oregon Experiment</em> (1975).
</p>

<aside className="notes">
- We showed a version of this in L6.

**Historical context:**
- Someone has claimed that this meme has existed for over 100 years
- Alexander used it to argue for participatory design
- His pattern language ideas later influenced software design patterns

**The core problem:** Requirements get lost in translation at every stage.

→ **Transition:** Why does this matter so much?
</aside>

</Slide>

<Slide>

## The Cost of Fixing Requirements Errors Grows Exponentially

<Img
  src="/img/lectures/web/l9-cost-curve.webp"
  prompt="A clean infographic showing the exponential cost of fixing requirements errors across project phases. Show a steep upward curve with labeled points:
- Requirements phase: 1x cost (small coin icon)
- Design phase: 5x cost (small stack of coins)
- Implementation: 10x cost (medium pile of coins)
- Testing: 20x cost (large pile of coins)
- After deployment: 100x cost (massive pile of gold bars)

Below the curve, show a chain reaction: a small crack in 'Requirements' leads to larger cracks in 'Design', which leads to broken code in 'Implementation', which leads to failed tests, which leads to angry users. The visual metaphor is that a small early mistake cascades into massive later problems."
  alt="Exponential cost curve showing: Requirements (1x), Design (5x), Implementation (10x), Testing (20x), Deployment (100x). Below, a crack in requirements cascades into larger problems at each stage."
/>

<aside className="notes">
**Why the cost grows:**
- Each phase builds on the previous one
- Misunderstood requirement → wrong design → wrong implementation → wrong tests
- Work already done may need to be discarded

**Worst case:** You never understand what users need and leave them unsatisfied.

→ **Transition:** So why is getting requirements so hard?
</aside>

</Slide>

<Slide>

## Requirements Analysis Is Not Just "Asking What They Want"

<p style={{fontSize: '0.85em'}}>
Users often:
</p>

<ul style={{fontSize: '0.8em'}}>
  <li><strong>Don't know</strong> what they want until they see it</li>
  <li><strong>Can't articulate</strong> their needs clearly</li>
  <li><strong>Focus on solutions</strong> instead of problems</li>
  <li><strong>Have conflicting needs</strong> with other stakeholders</li>
  <li><strong>Change their minds</strong> as they learn more</li>
</ul>

<p style={{fontSize: '0.85em', marginTop: '1em', fontWeight: 'bold', color: '#9370DB'}}>
Key insight: Users are experts in their domain. The goal is to engage them as design partners, not extract requirements like mining ore.
</p>

<aside className="notes">
**The fundamental insight:**
- Users understand their work better than we ever could
- We're not interrogating them—we're collaborating with them
- Christopher Alexander argued inhabitants should participate in design

**Approach shift:** From extraction to partnership.

→ **Transition:** Let's see the difference in approach...
</aside>

</Slide>

<Slide>

## Extractive vs. Participatory Approaches

<div style={{display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '1em', fontSize: '0.7em'}}>

<div style={{backgroundColor: 'rgba(255,100,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**Extractive Approach ❌**

> **Dev**: "What features do you need?"
>
> **Prof**: "I need automated grading."
>
> **Dev**: "OK, I'll build an autograder."
>
> *[Much Later...]*
>
> **Prof**: "This isn't what I wanted!"

</div>

<div style={{backgroundColor: 'rgba(100,255,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**Participatory Approach ✓**

> **Dev**: "Tell me about your grading challenges."
>
> **Prof**: "I spend weekends grading, students complain about inconsistency."
>
> **Dev**: "Can you show me the process?"
>
> **Prof**: *[Shows]* "I check correctness, style, approach..."
>
> **Dev**: "What if we automated mechanical parts so you could focus on pedagogy?"
>
> **Prof**: "That would change how I design assignments!"

</div>

</div>

<aside className="notes">
**The difference:**
- Extractive: One-way flow, developer assumes they understand
- Participatory: Two-way dialogue, shared discovery

**Key point:** Requirements emerge from collaboration. Neither party has the full solution alone.

→ **Transition:** Let's see how collaborative discovery works...
</aside>

</Slide>

<Slide>

## Collaborative Discovery Reveals Hidden Requirements

<div style={{fontSize: '0.75em', backgroundColor: 'rgba(147, 112, 219, 0.1)', padding: '1em', borderRadius: '8px'}}>

> **Professor**: "I need the system to be fair."
>
> **You**: "What does 'fair' mean to you?"
>
> **Professor**: "Students complain some TAs grade harder than others."
>
> **You**: "How would you like to address that?"
>
> **Professor**: "Maybe... standardize the grading somehow?"
>
> **You**: "Would a rubric help? Multiple graders? Statistical normalization?"
>
> **Professor**: "Oh! Actually, the real problem might be TAs interpret my rubric differently..."
>
> **You**: "What if TAs could see each other's grading on example submissions?"
>
> **Professor**: "Like a calibration exercise? That's brilliant!"

</div>

<p style={{fontSize: '0.8em', marginTop: '0.5em', color: '#4CAF50'}}>
✓ Neither party had this solution in mind—it emerged from collaboration.
</p>

<aside className="notes">
**Notice the evolution:**
- Vague value ("fairness") →
- Surface solution ("standardize") →
- Deeper understanding (rubric interpretation) →
- Novel solution (calibration exercises)

**Key skill:** Keep asking "why" and "what does that mean?"

→ **Transition:** This builds a shared understanding called a domain model...
</aside>

</Slide>

<Slide>

## Shared Language Enables Ongoing Collaboration

<ul style={{fontSize: '0.8em', marginTop: '0.5em'}}>
  <li>Professor stops saying "the system should be fair"</li>
  <li>Professor starts saying "we need inter-rater reliability"</li>
  <li>Developer stops thinking in databases</li>
  <li>Developer starts thinking in rubrics and learning outcomes</li>
</ul>

<aside className="notes">
**The domain model:**
- Shared understanding of concepts, relationships, and rules
- Allows all stakeholders to participate in design decisions
- Creates common language for the whole project

**Benefit:** Communication stays clear throughout the project.

→ **Transition:** Now let's talk about managing requirements risk...
</aside>

</Slide>

{/* ============================================ */}
{/* ARC 2: THREE DIMENSIONS OF RISK */}
{/* ============================================ */}

<Slide>

## Requirements Analysis Is Fundamentally About Managing Risk

<Img
  src="/img/lectures/web/l9-risk-dimensions.webp"
  prompt="A 3D coordinate system showing three axes of requirements risk, with the dangerous corner clearly marked.

X-AXIS: UNDERSTANDING - labeled from 'Crystal Clear' (origin, safe) to 'Totally Ambiguous' (far end, dangerous). Show gradient from green to red.

Y-AXIS: SCOPE - labeled from 'Tiny Change' (origin, safe) to 'Massive System' (far end, dangerous). Show gradient from green to red.

Z-AXIS: VOLATILITY - labeled from 'Rock Solid' (origin, safe) to 'Constantly Shifting' (far end, dangerous). Show gradient from green to red.

The ORIGIN corner (0,0,0) where all three are low is colored bright green with a happy face or checkmark, labeled 'Safe Zone - Low Risk'.

The FAR CORNER where all three axes are at maximum is colored bright red with warning symbols, flames, or skull, labeled 'DANGER ZONE - Maximum Risk'. This corner should be visually alarming.

Show a few example requirement dots plotted in the space:
- A green dot near origin: 'Record UTC timestamp'
- A yellow dot in middle: 'Add regrade workflow'
- A red dot near danger corner: 'ML-powered partial credit'

The visual clearly conveys that requirements closer to the danger corner need the most attention and carry the highest risk."
  alt="3D coordinate system with three risk axes: Understanding (clear to ambiguous), Scope (tiny to massive), Volatility (stable to shifting). Origin is green 'Safe Zone', far corner is red 'Danger Zone'. Example requirements plotted: timestamp (green/safe), regrade workflow (yellow/medium), ML grading (red/dangerous)."
/>

<p style={{fontSize: '0.85em', marginTop: '0.5em'}}>
Every requirement carries risks. Understanding these risks helps you focus effort where it matters most.
</p>

<aside className="notes">
**The framework:**
- Not all requirements are equal risk
- Three dimensions help evaluate each requirement
- High-risk requirements need more attention

**Goal:** Prioritize effort on highest-risk requirements.

→ **Transition:** Let's examine each dimension...
</aside>

</Slide>

<Slide>

## Risk Dimension 1: Understanding

<p style={{fontSize: '0.9em', fontWeight: 'bold', color: '#9370DB'}}>
How well do we understand what's needed?
</p>

<div style={{display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '1em', fontSize: '0.75em'}}>

<div style={{backgroundColor: 'rgba(100,255,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**Low Understanding Risk ✓**

> "The system shall record the UTC timestamp when a submission is received."

Clear, unambiguous, everyone knows what a timestamp is.

</div>

<div style={{backgroundColor: 'rgba(255,100,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**High Understanding Risk ❌**

> "The system shall ensure grading quality through meta-reviews."

What's a meta-review? Who does it? When? What constitutes "quality"?

</div>

</div>

<aside className="notes">
**Low risk:** Technical terms with clear definitions
**High risk:** Vague terms that different people interpret differently

**Watch for:** Same word meaning different things to different stakeholders.

→ **Transition:** Let's see how interpretations can differ...
</aside>

</Slide>

<Slide>

## Different Stakeholders Interpret the Same Term Differently

<Img
  src="/img/lectures/web/l9-meta-review-interpretations.webp"
  prompt="An illustration showing how three different stakeholders interpret 'meta-review' completely differently. Show three thought bubbles emerging from three people:

1. PROFESSOR's thought bubble: Shows senior TAs doing spot-checks on random samples after grading is complete, with a quality seal of approval

2. TA's thought bubble: Shows two graders reviewing the same assignment together during grading, with discussion bubbles showing them reconciling differences

3. STUDENT's thought bubble: Shows an appeals process with a judge figure reviewing their contested grade, with scales of justice

All three are looking at the same requirement document that says 'meta-reviews', but each imagines something completely different. The illustration highlights how one vague term can mean totally different things."
  alt="Three people reading 'meta-reviews' requirement with different interpretations: Professor sees spot-checks by senior TAs, TA sees peer review during grading, Student sees appeals process with impartial review."
/>

<p style={{fontSize: '0.8em', marginTop: '0.5em', color: '#FF9800'}}>
⚠ One vague term, three completely different interpretations.
</p>

<aside className="notes">
**The danger:**
- Each stakeholder has a valid interpretation
- Without clarification, someone will be disappointed
- Could lead to building the wrong thing

**Solution:** Ask for concrete examples, create prototypes, define terms explicitly.

→ **Transition:** Second dimension: Scope...
</aside>

</Slide>

<Slide>

## Risk Dimension 2: Scope

<p style={{fontSize: '0.9em', fontWeight: 'bold', color: '#9370DB'}}>
How much are we trying to build?
</p>

<div style={{display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '1em', fontSize: '0.75em'}}>

<div style={{backgroundColor: 'rgba(100,255,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**Low Scope Risk ✓**

> "Students have a fixed number of late tokens, each allowing a 24-hour extension."

Small, isolated change with clear boundaries.

</div>

<div style={{backgroundColor: 'rgba(255,100,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**High Scope Risk ❌**

> "Students can request regrades for any grading decision, which can be escalated to instructors."

Sounds like one feature, but what does it really mean?

</div>

</div>

<aside className="notes">
**Low scope:** Clear boundaries, few edge cases
**High scope:** Hidden complexity, many interconnected decisions

**Key insight:** Scope risk isn't just feature count—it's interdependencies and edge cases.

→ **Transition:** Let's unpack that "one feature"...
</aside>

</Slide>

<Slide>

## Poll: Regrade Requests

If you were asked to add regrade requests to an online grading system,
what questions would you ask the requester?

<PollSlide username="espertus" />

</Slide>

<Slide>

## One Feature Hides Dozens of Decisions - Regrades

<div style={{display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '0.5em', fontSize: '0.65em'}}>

<div>

**Visible requirements:**
- Students can request regrades
- Requests include justification
- Original grader reviews requests
- Decisions can be appealed

</div>

<div className="fragment">

**Hidden requirements:**
- Who can request? (Student only? Team members?)
- When can they request? (Immediately? Cooling period? Before finals?)
- How many requests allowed? (Per assignment? Per semester?)
- What's the escalation path? (Grader → Meta-grader → Instructor?)
- What prevents gaming? (Shopping for easy grader?)
- How are groups handled? (One request per group? Individual?)
- What about concurrent modifications?
- What audit trail is needed?

</div>

</div>

<p className="fragment" style={{fontSize: '0.8em', marginTop: '0.5em', color: '#FF9800'}}>
⚠ Each question represents additional scope. What seemed like one feature is actually dozens of interconnected decisions.
</p>

<aside className="notes">
**The iceberg:**
- Visible requirements are just the tip
- Hidden requirements emerge during analysis
- Each "question" is work that must be done

**Professional skill:** Negotiating scope when requirements exceed resources.

→ **Transition:** Third dimension: Volatility...
</aside>

</Slide>

<Slide>

## Risk Dimension 3: Volatility

<p style={{fontSize: '0.9em', fontWeight: 'bold', color: '#9370DB'}}>
How likely are requirements to change?
</p>

<div style={{display: 'grid', gridTemplateColumns: '1fr 1fr', gap: '1em', fontSize: '0.75em'}}>

<div style={{backgroundColor: 'rgba(100,255,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**Low Volatility Risk ✓**

> "Use the university's standard letter grading scale (A, A-, B+, B, B-, ...)"

Stable for decades, unlikely to change.

</div>

<div style={{backgroundColor: 'rgba(255,100,100,0.1)', padding: '0.5em', borderRadius: '8px'}}>

**High Volatility Risk ❌**

> "Integrate with the university's new AI-powered plagiarism detector currently in contract negotiations with three vendors."

Unstable API, multiple vendors, external organization control.

</div>

</div>

<aside className="notes">
**Low volatility:** Established standards, regulatory requirements
**High volatility:** New technology, external dependencies, political factors

**Professional skill:** Adaptability when requirements inevitably change.

→ **Transition:** Let's see how volatile requirements evolve...
</aside>

</Slide>

<Slide>

## Volatile Requirements Evolve Unpredictably

<Img
  src="/img/lectures/web/l9-volatile-rollercoaster.webp"
  prompt="A dramatic illustration of a rollercoaster track representing a project timeline for an 'AI Plagiarism Detector' feature. The track STARTS at a grand building labeled 'CHANCELLOR'S OFFICE' at Week 1, where a smiling chancellor waves goodbye to a developer boarding the cart. A sign reads 'Simple yes/no plagiarism check - should be easy!' The chancellor looks confident and carefree.

The track then goes through increasingly chaotic sections:
- Week 3: A loop-de-loop labeled 'Similarity %! Highlighting! Corpus comparison!'
- Week 5: A steep climb with warning signs for 'LEGAL REVIEW', 'Student Consent Required', 'Azure Hosting Mandate'
- Week 7: The track suddenly stops at a brick wall labeled 'AI ETHICS COMMITTEE - FEATURE ON HOLD'. The developer looks shocked, cart screeching to a halt.
- Week 10: The track resumes but loops ALL THE WAY BACK to the same Chancellor's Office, now with the chancellor looking confused/sheepish behind a desk piled with committee reports. A sign says 'Sub-committee forming... let's revisit the requirements.'

The irony is visual and clear: the journey started AND ended at the Chancellor's Office - the same stakeholder who gave the vague initial requirement is now the source of all the volatility. The developer in the cart looks exhausted and exasperated, having gone through all of this chaos only to end up back where they started. Include flying paperwork, committee meeting signs, and bureaucratic red tape tangled in the track."
  alt="Rollercoaster timeline starting at Chancellor's Office (Week 1, simple request) through loops (Week 3), steep climbs (Week 5 legal/compliance), crashes into Ethics Committee wall (Week 7), then loops back to the SAME Chancellor's Office (Week 10) - illustrating that the source of volatility was the original stakeholder all along."
/>

<p style={{fontSize: '0.8em', marginTop: '0.5em'}}>
<strong>Sources of volatility:</strong> External dependencies, regulatory requirements, political factors, technical uncertainty, market changes.
</p>

<aside className="notes">
**Notice:**
- Each revision adds complexity
- External factors beyond your control
- What seemed simple becomes impossible

**Strategy:** Isolate volatile requirements behind interfaces, build stable core first.

→ **Transition:** What happens when requirements are risky on all dimensions?
</aside>

</Slide>

<Slide>

## High-Risk Requirements Score High on Multiple Dimensions

<p style={{fontSize: '0.85em'}}>
Consider this requirement:
</p>

<div style={{backgroundColor: 'rgba(255,100,100,0.1)', padding: '0.5em', borderRadius: '8px', fontSize: '0.85em'}}>

> "The system shall use machine learning to automatically assign partial credit in a way that's pedagogically sound and legally defensible."

</div>

<div style={{fontSize: '0.7em', marginTop: '0.5em'}}>

| Dimension | Risk Level | Why |
|-----------|------------|-----|
| **Understanding** | HIGH | What is "pedagogically sound"? "Legally defensible"? What ML? |
| **Scope** | HIGH | Training data, scaling to new assignments, ML pipeline, evaluation, legal review, appeals system |
| **Volatility** | HIGH | ML evolves, legal requirements change, pedagogy is debated |

</div>

<aside className="notes">
**The danger zone:**
- High on all three dimensions
- This requirement will cause problems

**Options:** Clarify, Simplify, Stabilize, Defer, or Eliminate.

→ **Transition:** Now let's talk about who we gather requirements from...
</aside>

</Slide>

{/* ============================================ */}
{/* ARC 3: STAKEHOLDER IDENTIFICATION */}
{/* ============================================ */}

<Slide>

## Poll: Stakeholders

Who are stakeholders for an online grading system?

<PollSlide username="espertus" />
 <p style={{fontSize: '0.85em', color: '#666', marginTop: '0.5em'}}>
 Give short answers for a word cloud.
 </p>

</Slide>

<Slide>

## A Stakeholder Is Anyone Affected By or Affecting the System

<Img
  src="/img/lectures/web/l9-stakeholders.webp"
  prompt="An illustration showing the different types of stakeholders for a grading system, arranged in concentric circles.

INNER CIRCLE (Primary - Direct Users):
- Students (icon: graduation cap)
- TAs/Graders (icon: red pen)
- Instructors (icon: lectern)

MIDDLE CIRCLE (Secondary - Indirect Users):
- Meta-graders/Senior TAs (icon: magnifying glass over grade)
- Department Administrator (icon: filing cabinet)
- IT Support (icon: server)

OUTER CIRCLE (Hidden - Often Forgotten):
- Parents (icon: phone)
- Future Employers (icon: briefcase)
- Accreditation Bodies (icon: certificate/seal)
"
  alt="Concentric circles of stakeholders: Inner (Students, TAs, Instructors), Middle (Meta-graders, Administrators, IT), Outer (Parents, Future Employers, Accreditation). Shows stakeholders extend beyond obvious users."
/>

<p style={{fontSize: '0.85em', marginTop: '0.5em'}}>
Missing a key stakeholder is like designing a building without talking to the people who will live in it.
</p>

<aside className="notes">
**Three categories:**
- Primary: Direct users
- Secondary: Indirect users
- Hidden: Often forgotten but important

**Key insight:** Each missing perspective is compounding risk.

→ **Transition:** Let's examine primary stakeholders...
</aside>

</Slide>

<Slide>

## Primary Stakeholders Have Different Values and Fears

<div style={{fontSize: '0.65em'}}>

| Stakeholder | Primary Concern | Values | Fears |
|-------------|-----------------|--------|-------|
| **Students** | "Will I get the grade I deserve?" | Fairness, transparency, timeliness | Biased grading, lost submissions, harsh TA |
| **Graders (TAs)** | "Can I grade fairly without losing my weekend?" | Efficiency, accuracy, workload balance | Overwhelming workload, hostile regrades |
| **Instructors** | "Are students learning and evaluated fairly?" | Educational outcomes, integrity, oversight | Grade complaints, inconsistent grading |

</div>

<aside className="notes">
**Notice the different perspectives:**
- Students care about outcomes and fairness
- TAs care about workload and protection
- Instructors care about learning and oversight

**Each has legitimate concerns that may conflict with others.**

→ **Transition:** Don't forget secondary and hidden stakeholders...
</aside>

</Slide>

<Slide>

## Hidden Stakeholders Can Derail Projects

<Img
  src="/img/lectures/web/l9-hidden-stakeholders.webp"
  prompt="An illustration showing hidden stakeholders emerging from shadows to affect a project. Show a software project (represented as a building under construction) with obvious stakeholders (students, TAs, instructors) visible and working.

From the shadows or edges, hidden stakeholders emerge:
- PARENTS calling the dean demanding grade explanations (phone icon)
- FUTURE EMPLOYERS looking confused at inconsistent transcripts (briefcase icon)
- ACCREDITATION BODY stamping 'AUDIT REQUIRED' on the building (official seal)
- REGULATORS appearing with FERPA/GDPR compliance checklists

The message: these stakeholders don't interact with the system directly but can shut it down or demand major changes. A speech bubble says 'We weren't consulted!' The overall tone should convey surprise and the importance of proactive identification."
  alt="Software project with visible stakeholders working, while hidden stakeholders emerge from shadows: Parents calling dean, Employers confused by transcripts, Accreditation body demanding audit, Regulators with compliance checklists. Speech bubble: We weren't consulted!"
/>

<ul style={{fontSize: '0.8em', marginTop: '0.5em'}}>
  <li><strong>Parents:</strong> May call demanding grade explanations</li>
  <li><strong>Future Employers:</strong> Rely on grades as competence signals</li>
  <li><strong>Accreditation Bodies:</strong> Can shut down programs that don't meet standards</li>
</ul>

<aside className="notes">
**Why they matter:**
- Parents influence students and administrators
- Employers are affected by grade integrity
- Accreditation can shut down entire programs

**Pro tip:** Identify future stakeholders for long-term risk management.

→ **Transition:** Stakeholder interests often conflict...
</aside>

</Slide>

<Slide>

## Poll: Stakeholder Conflicts

Identify a conflict between students and graders.

<PollSlide username="espertus" />
 <p style={{fontSize: '0.85em', color: '#666', marginTop: '0.5em'}}>
 </p>

</Slide>

<Slide>

## Stakeholder Interests Often Directly Conflict

<div style={{fontSize: '0.75em'}}>

| Conflict | Stakeholder 1 | Stakeholder 2 |
|----------|--------------|---------------|
| **Regrades** | Students want unlimited requests for fairness | TAs want protection from frivolous requests |
| **Analytics** | Instructors want to audit all student code for trojan horses | Students want privacy of mistakes |
| **Automation** | TAs want fully automated grading | Instructors want nuanced evaluation |
| **Feedback timing** | Fast students want immediate feedback | TAs need to batch for efficiency |
| **Audit trails** | Administrators want detailed logs | TAs want quick, simple grade entry |

</div>

<p style={{fontSize: '0.8em', marginTop: '0.5em', color: '#9370DB'}}>
Resolving conflicts is a negotiation skill—understanding what each party truly needs and finding creative solutions.
</p>

<aside className="notes">
**The challenge:**
- Every row is a real conflict you must resolve
- No one is wrong—each has legitimate concerns

**Key skill:** Find creative solutions that address underlying needs.

→ **Transition:** For example, balancing regrade needs...
</aside>

</Slide>

<Slide>

## Balancing Competing Stakeholder Needs

<p style={{fontSize: '0.85em'}}>
<strong>Example: Designing Pawtograder's regrade request feature</strong>
</p>

<div style={{fontSize: '0.8em'}}>

| Stakeholder | Need | Solution Element |
|-------------|------|------------------|
| **Student** | Easy ability to request regrades | <span className="fragment">Simple request form in Pawtograder</span> |
| **TA** | Protection from frivolous requests | <span className="fragment">7-day time limit after grades posted</span> |
| **Instructor** | Oversight of grading quality | <span className="fragment">3-day appeal window to instructor if unsatisfied</span> |
| **Administrator** | Compliance and audit trail | <span className="fragment">Complete log of all regrade activities</span> |

</div>

<p className="fragment" style={{fontSize: '0.8em', marginTop: '0.5em', color: '#4CAF50'}}>
✓ A good solution addresses all concerns, not just the loudest voice.
</p>

<aside className="notes">
**Notice:**
- No stakeholder gets everything they want
- But each stakeholder's core need is addressed
- This requires understanding *why* they want what they want

**Key insight:** Requirements analysis reveals the "why" behind requests.

→ **Transition:** Now let's talk about methods to elicit requirements...
</aside>

</Slide>

{/* ============================================ */}
{/* ARC 4: ELICITATION METHODS */}
{/* ============================================ */}

<Slide>

## Different Methods Reveal Different Information

<Img
  src="/img/lectures/web/l9-elicitation-methods.webp"
  prompt="An illustration showing six different requirements elicitation methods, each represented as a different tool or lens that reveals different aspects of requirements:

1. INTERVIEWS - A conversation bubble with back-and-forth dialogue
2. OBSERVATION - An eye watching someone work at a computer
3. WORKSHOPS - A group of people around a whiteboard with sticky notes
4. PROTOTYPES - A hand sketching a quick interface mockup
5. DOCUMENT ANALYSIS - A magnifying glass over policy documents
6. SCENARIOS - A storyboard showing a user journey

In the center, these methods converge on a complete requirements document. The message: each method has blind spots, so combine multiple methods. Some methods are better for different situations."
  alt="Six elicitation methods as different lenses: Interviews (dialogue), Observation (eye watching work), Workshops (group with sticky notes), Prototypes (hand sketching), Document Analysis (magnifying glass on policies), Scenarios (storyboard). They converge on complete requirements."
/>

<aside className="notes">
**The toolkit:**
- No single method is sufficient
- Each reveals different aspects
- Combine methods strategically

**Goal:** Understand what people say, what they do, what they need, and what constrains them.

→ **Transition:** Let's examine each method...
</aside>

</Slide>

<Slide>

## Method 1: Interviews Reveal What Stakeholders Say

<div style={{fontSize: '0.7em', backgroundColor: 'rgba(147, 112, 219, 0.1)', padding: '0.5em', borderRadius: '8px'}}>

> **Q**: Walk me through grading a typical assignment from start to finish.
>
> "I download submissions from Canvas, run them locally, open a spreadsheet, test each function manually..."
>
> **Q**: What takes the most time?
>
> "Setting up the test environment for each student's code, and writing the same feedback over and over."
>
> **Q**: Tell me about the last time grading went really badly.
>
> "A student submitted at 11:59 PM, I graded at midnight, but they claimed they resubmitted at 12:01 AM and I graded the wrong version..."

</div>

<p style={{fontSize: '0.75em', marginTop: '0.5em'}}>
<strong>Techniques:</strong> Open-ended questions, critical incident technique, probing, silence
</p>

<aside className="notes">
**Interview techniques:**
- "Tell me about..." not "Do you..."
- "Describe a time when things went wrong"
- Ask "why" and "can you give an example?"

**Pitfall:** Leading questions, technical jargon, accepting vague answers.

→ **Transition:** But what people say isn't always what they do...
</aside>

</Slide>

<Slide>

## Method 2: Observation Reveals What Stakeholders Actually Do

<div style={{fontSize: '0.65em'}}>

| Time | Observed Activity |
|------|-------------------|
| 2:00 PM | TA logs into Canvas, downloads submission ZIP |
| 2:05 PM | Discovers some submissions are .tar.gz (unexpected!) |
| 2:08 PM | Writes quick script to handle multiple archive formats |
| 2:15 PM | Opens first submission, realizes it's Python 2 not Python 3 |
| 2:18 PM | Sets up separate testing environment for Python 2 |
| 2:20 PM | Manually copies test cases from assignment PDF |
| 2:30 PM | Student order in spreadsheet doesn't match Canvas |
| 2:37 PM | Phone notification interrupts flow |
| 2:38 PM | Loses track of which submission was being graded |

</div>

<p style={{fontSize: '0.8em', marginTop: '0.5em', color: '#FF9800'}}>
⚠ File format issues, manual test copying, and interruptions weren't mentioned in interviews.
</p>

<aside className="notes">
**Key discoveries:**
- Problems users don't think to mention
- Workarounds they've normalized
- Context switching and interruptions

**Insight:** What people do often differs from what they say.

→ **Transition:** Workshops bring stakeholders together...
</aside>

</Slide>

<Slide>

## Method 3: Workshops Resolve Conflicts Early

<Img
  src="/img/lectures/web/l9-workshop.webp"
  prompt="An illustration of a requirements workshop in progress. Show a diverse group around a table/whiteboard:
- Sticky notes grouped into clusters (affinity mapping)
- Dot voting happening (people placing dots on sticky notes)
- Two people in animated discussion resolving a conflict
- A facilitator managing the session

On the whiteboard, show clusters labeled: 'Need justification for regrades', 'Limit number of requests', 'Anonymous grading to prevent bias'

Include a negotiation happening:
- Student: 'We need unlimited regrades for fairness'
- TA: 'That would overwhelm us during finals'
- Compromise emerging: 'Three regrades per semester, serious errors don't count against limit'

The visual should convey productive collaboration and conflict resolution."
  alt="Requirements workshop: diverse group around whiteboard with sticky note clusters, dot voting in progress, two people negotiating. Whiteboard shows requirement clusters. Dialogue bubble shows student-TA negotiation reaching compromise on regrade limits."
/>

<p style={{fontSize: '0.8em', marginTop: '0.5em'}}>
<strong>Benefits:</strong> Stakeholders hear each other directly, creative solutions emerge, conflicts surface and resolve early, builds buy-in.
</p>

<aside className="notes">
**Workshop activities:**
- Individual brainstorming
- Affinity mapping (grouping ideas)
- Conflict resolution
- Dot voting for priorities

**Key benefit:** Stakeholders negotiate directly rather than through you.

→ **Transition:** Prototypes make requirements concrete...
</aside>

</Slide>

<Slide>

## Method 4: Prototypes Make Abstract Ideas Concrete

<Img
  src="/img/lectures/web/l9-prototype-mirror-window.webp"
  prompt="A split comparison illustration showing two stages of requirements gathering.

LEFT SIDE - 'STAGE 1: THE MIRROR' (with a small green checkmark):
A young male developer holds up a literal mirror toward a female professor/stakeholder. The mirror reflects back her words for confirmation: 'So you want students to request regrades?' The professor nods in agreement. This is useful for initial understanding. Label: 'Reflecting back confirms understanding - good start!'

RIGHT SIDE - 'STAGE 2: THE WINDOW' (with a larger green checkmark):
The same young male developer holds up a window frame containing a rough paper prototype/wireframe sketch of a regrade request form. The female professor is leaning forward, engaged, pointing at specific elements with animated gestures. Speech bubbles show concrete feedback: 'Wait, where do I see my original submission?' 'Can I request multiple problems?' 'How do I know it was received?' She is actively discovering requirements she didn't know she had. Label: 'But prototypes reveal what words cannot'

The visual progression shows: mirrors are helpful for Stage 1 (confirming you heard correctly), but you need to progress to Stage 2 (prototypes) to discover the deeper requirements. An arrow or progression indicator between the two sides shows this is a sequence, not just a comparison. The paper prototype can be rough/sketchy - that's the point. Include some pencil marks and crossed-out elements to show it's meant to be changed."
  alt="Split image showing two stages: Stage 1 (Mirror) - developer reflects words back to professor for confirmation, useful starting point. Stage 2 (Window) - developer shows paper prototype, professor discovers new requirements she didn't know she had. Progression from confirming understanding to sparking deeper discovery."
/>

<aside className="notes">
**Why prototypes work:**
- Users can react to something concrete
- Reveals assumptions neither party knew they had
- Cheap to change paper sketches

**Even crude prototypes reveal deep requirements.**

→ **Transition:** Documents reveal constraints...
</aside>

</Slide>

<Slide>

## Method 5: Document Analysis Reveals Hidden Constraints

<div style={{fontSize: '0.8em'}}>

**Documents to analyze:**
- Grading rubrics → evaluation criteria and point distributions
- Email complaints → common pain points and misunderstandings
- Grade appeal forms → existing process and requirements
- TA training materials → hidden complexities
- University policies → compliance constraints
- Previous syllabi → evolution of grading policies

</div>

<div className="fragment" style={{backgroundColor: 'rgba(255,200,100,0.2)', padding: '0.5em', borderRadius: '8px', fontSize: '0.75em', marginTop: '0.5em'}}>

**Example from CS 3100 syllabus:**
> "All regrade requests must be submitted within 7 days from your receipt of the graded work. If your regrade request is closed and you feel that the response was not satisfactory, you may appeal to the instructor via Pawtograder within 3 days."

**Requirements discovered:** 7-day time limit after receipt (not posting), written form via Pawtograder, escalation path to instructor, 3-day appeal window, implies graders are not instructors.

</div>

<aside className="notes">
**Why documents matter:**
- Reveal formal constraints
- Show how current processes work
- Often forgotten during interviews

**Tip:** Look for email complaints—they're unfiltered feedback.

→ **Transition:** Scenarios test completeness...
</aside>

</Slide>

<Slide>

## Method 6: Scenarios Reveal Edge Cases

<div style={{fontSize: '0.7em', backgroundColor: 'rgba(147, 112, 219, 0.1)', padding: '0.5em', borderRadius: '8px'}}>

**Scenario: End-of-Semester Grade Dispute**

Sarah, a senior, needs a B+ to keep her scholarship. She receives a B after the final assignment. Checking her grades, she notices Assignment 3 seems low. She reviews the rubric and believes her recursive solution was incorrectly marked wrong.

She initiates a regrade request on December 15th. The original TA, Tom, is on winter break. The system escalates to the instructor, who must respond within 48 hours due to grade submission deadlines. The instructor realizes they never saw Sarah's original grade—they need visibility into all grading decisions, not just escalations...

</div>

<p style={{fontSize: '0.75em', marginTop: '0.5em'}}>
<strong>Requirements revealed:</strong> Instructor visibility into all grades (not just appeals!), escalation when grader unavailable, urgency handling, time-sensitive responses.
</p>

<aside className="notes">
**Why scenarios work:**
- Force you to think through edge cases
- Reveal missing requirements
- Test completeness of current understanding

**Tip:** Write scenarios for things going wrong, not just happy paths.

→ **Transition:** Combine all methods for best results...
</aside>

</Slide>

<Slide>

## Combine Methods for Complete Requirements

<div style={{fontSize: '0.8em'}}>

1. **Start broad:** Interview stakeholders for general needs
2. **Get specific:** Observe actual work to see hidden complexities
3. **Resolve conflicts:** Run workshops with multiple stakeholders
4. **Validate understanding:** Test with prototypes
5. **Check constraints:** Analyze policy documents
6. **Verify completeness:** Walk through detailed scenarios

</div>

<p style={{fontSize: '0.85em', marginTop: '1em', fontWeight: 'bold', color: '#FF9800'}}>
Red flags to watch for:
</p>

<ul style={{fontSize: '0.75em'}}>
  <li>Only talking to managers (they don't do the actual work)</li>
  <li>Leading questions (you'll hear what you want to hear)</li>
  <li>Single method (each method has blind spots)</li>
  <li>Missing stakeholders (the quiet ones often have critical needs)</li>
</ul>

<aside className="notes">
**Key insight:**
- Requirements elicitation is not a phase—it's ongoing
- As stakeholders see prototypes, they remember things
- Requirements evolve as understanding deepens

**Remember:** No single method is sufficient.

→ **Transition:** Let's summarize...
</aside>

</Slide>

{/* ============================================ */}
{/* KEY TAKEAWAYS */}
{/* ============================================ */}

<Slide>

## Key Takeaways

<ul style={{fontSize: '0.8em'}}>
  <li><strong>Requirements analysis</strong> bridges vague needs to concrete specifications</li>
  <li><strong>Three risk dimensions:</strong> Understanding, Scope, and Volatility</li>
  <li><strong>Stakeholder analysis</strong> identifies everyone affected, including hidden stakeholders</li>
  <li><strong>Multiple elicitation methods</strong> needed—each has blind spots</li>
  <li><strong>Participatory approach:</strong> Engage users as design partners, not just requirement sources</li>
</ul>

<p style={{fontSize: '0.85em', marginTop: '1em', fontWeight: 'bold', color: '#9370DB'}}>
The key insight: Requirements analysis isn't asking "what do you want?"—it's understanding problems deeply enough to design solutions stakeholders didn't know were possible.
</p>

<aside className="notes">
**For your assignments:**
- Identify stakeholders and their values
- Assess risk on all three dimensions
- Use multiple elicitation methods
- Build shared understanding with users

**Looking ahead:**
- Next lectures apply these ideas to design
- Requirements inform all architectural decisions

</aside>

</Slide>

<Slide>
## Bonus Slide

<Img src="/img/lectures/web/l9-customer-needs-cat.jpg"
 alt="Left panel has title 'Product features' and shows a fancy cat tree. Right panel has title 'User Needs' and shows a cat in a cardbox box."
 />
 Source: [@_yes_but](https://www.instagram.com/_yes_but/)
 </Slide>

</RevealJS>
